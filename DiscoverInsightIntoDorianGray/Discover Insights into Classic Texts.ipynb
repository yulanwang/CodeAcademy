{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74ff79ad",
   "metadata": {},
   "source": [
    "# Discover Insights into Classic Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0bb37",
   "metadata": {},
   "source": [
    "Novels and text contain insights into ideologies and places that are often originally unknown to the reader. By reading a written piece, you uncover the opinions of the author on their chosen topic and come to understand both the topic and how the author thinks.\n",
    "\n",
    "In this project you will perform a natural language parsing analysis to gain deeper insight into one of two famous and often discussed novels in the public domain: <a href=\"http://www.gutenberg.org/ebooks/174\" target=\"_blank\" rel=\"noopener noreferrer\">Oscar Wilde's _The Picture of Dorian Gray_</a> or <a href=\"http://www.gutenberg.org/ebooks/6130\" target=\"_blank\" rel=\"noopener noreferrer\"> Homer's _The Iliad!_</a> Fear not if you haven't heard or read the novels, one of the beauties of natural language parsing with regular expressions is the ability to gain insight into lengthy pieces of text without a formal read!\n",
    "\n",
    "By the end of this project, you will find out the main topics of discussion in the novel of your choosing and can begin to discern some of the author's thoughts and beliefs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf3caef",
   "metadata": {},
   "source": [
    "## Import and Preprocess Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359007fa",
   "metadata": {},
   "source": [
    "1. Given to you in the downloadable kit are text files for _The Picture of Dorian Gray_, named `dorian_gray.txt`, and _The Iliad_, named `the_iliad.txt`, sourced from <a href=\"https://www.gutenberg.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Project Gutenberg</a>. Import the text of your choosing, convert it to lowercase, and name it `text` using the following line of code.\n",
    "\n",
    "   ```py\n",
    "   text = open(\"_______.txt\",encoding='utf-8').read().lower()\n",
    "   ```\n",
    "   \n",
    "   Replace the blank with the name of the text file for the novel you choose to analyze!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b207551d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mbs4\u001b[39;00m\n\u001b[1;32m      2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maveraged_perceptron_tagger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas, sklearn, matplotlib, seaborn, nltk, bs4\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk import pos_tag, RegexpParser\n",
    "import import_ipynb\n",
    "from tokenize_words import word_sentence_tokenize\n",
    "from chunk_counters import np_chunk_counter, vp_chunk_counter\n",
    "\n",
    "# import text of choice here\n",
    "# I chose to analyze The Picture of Dorian Gray\n",
    "text = open(\"dorian_gray.txt\",encoding='utf-8').read().lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0010d6bd",
   "metadata": {},
   "source": [
    "2. With the text imported, now you need to split the text into individual sentences and then individual words. This allows you to perform a sentence-by-sentence parsing analysis!\n",
    "\n",
    "   Provided to you in the downloadable kit is a customized function `word_sentence_tokenize()` that will sentence tokenize a text and then word tokenize each sentence, returning a list of word tokenized sentences. Call the function with `text` as an argument and save the result to a variable named `word_tokenized_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc47040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence and word tokenize text here\n",
    "word_tokenized_text = word_sentence_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11336c02",
   "metadata": {},
   "source": [
    "3. Save any word tokenized sentence in `word_tokenized_text` to a variable named `single_word_tokenized_sentence`. Print `single_word_tokenized_sentence` as a check to visualize what you have done so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5ce7fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no', 'artist', 'is', 'ever', 'morbid', '.']\n"
     ]
    }
   ],
   "source": [
    "# store and print any word tokenized sentence here\n",
    "# I chose the sentence at index 19:\n",
    "single_word_tokenized_sentence = word_tokenized_text[19]\n",
    "print(single_word_tokenized_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191d1eb1",
   "metadata": {},
   "source": [
    "## Part-of-speech Tag Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e5d1f",
   "metadata": {},
   "source": [
    "4. Next you can part-of-speech tag each sentence to allow for syntax parsing! Begin by creating a list named `pos_tagged_text` that will hold each part-of-speech tagged sentence from the novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0603deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list to hold part-of-speech tagged sentences here\n",
    "pos_tagged_text = list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43a24a",
   "metadata": {},
   "source": [
    "5. Loop through each word tokenized sentence in `word_tokenized_text` and part-of-speech tag each sentence using `nltk`'s `pos_tag()` function. Append the result to `pos_tagged_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b39a4538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a for loop through each word tokenized sentence here\n",
    "for word_tokenized_sentence in word_tokenized_text:\n",
    "  # part-of-speech tag each sentence and append to list of pos-tagged sentences here\n",
    "    pos_tagged_text.append(pos_tag(word_tokenized_sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8205d88f",
   "metadata": {},
   "source": [
    "6. Save any part-of-speech tagged sentence in `pos_tagged_text` to a variable named `single_pos_sentence`. Print `single_pos_sentence` as a check to visualize what you have done so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8af95d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('no', 'DT'), ('artist', 'NN'), ('is', 'VBZ'), ('ever', 'RB'), ('morbid', 'VBN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# store and print any part-of-speech tagged sentence here\n",
    "single_pos_sentence = pos_tagged_text[19]\n",
    "print(single_pos_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be552549",
   "metadata": {},
   "source": [
    "## Chunk Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c8554",
   "metadata": {},
   "source": [
    "7. Now that you have part-of-speech tagged your text, you can move on to syntax parsing!\n",
    "\n",
    "   Begin by defining a piece of chunk grammar `np_chunk_grammar` that will chunk a noun phrase. Remember, a noun phrase consists of an optional determiner `DT`, followed by any number of adjectives `JJ`, followed by a noun `NN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4333ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define noun phrase chunk grammar here\n",
    "np_chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a794295",
   "metadata": {},
   "source": [
    "8. Create a `nltk` `RegexpParser` object named `np_chunk_parser` using the noun phrase chunk grammar you defined as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c985dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create noun phrase RegexpParser object here\n",
    "\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d08787c",
   "metadata": {},
   "source": [
    "9. Define a piece of chunk grammar named `vp_chunk_grammar` that will chunk a verb phrase of the following form: noun phrase, followed by a verb `VB`. followed by an optional adverb `RB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f807a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define verb phrase chunk grammar here\n",
    "vp_chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553fbb0",
   "metadata": {},
   "source": [
    "10. Create a `nltk` `RegexpParser` object named `vp_chunk_parser` using the verb phrase chunk grammar you defined as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ee7f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create verb phrase RegexpParser object here\n",
    "vp_chunk_parser = RegexpParser(vp_chunk_grammar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51341fa",
   "metadata": {},
   "source": [
    "11. Create two empty lists `np_chunked_test` and `vp_chunked_test` that will hold the chunked sentences from your text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "047c8383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list to hold noun phrase chunked sentences and a list to hold verb phrase chunked sentences here\n",
    "np_chunked_text = list()\n",
    "vp_chunked_text = list()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf6425",
   "metadata": {},
   "source": [
    "12. Loop through each part-of-speech tagged sentence in `pos_tagged_text` and noun phrase chunk each sentence using your `RegexpParser`'s `.parse()` method. Append the result to `np_chunked_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "162d0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a for loop through each pos-tagged sentence here\n",
    "for pos_tagged_sentence in pos_tagged_text:\n",
    "  # chunk each sentence and append to list here\n",
    "    np_chunked_text.append(np_chunk_parser.parse(pos_tagged_sentence))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cab095",
   "metadata": {},
   "source": [
    "13. Within the same loop you defined in the previous task, verb phrase chunk each part-of-speech tagged sentence using your `RegexpParser`'s `.parse()` method. Append the result to `vp_chunked_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a7d0e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a for loop through each pos-tagged sentence here\n",
    "for pos_tagged_sentence in pos_tagged_text:\n",
    "  # chunk each sentence and append to lists here\n",
    "    vp_chunked_text.append(vp_chunk_parser.parse(pos_tagged_sentence))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade8062",
   "metadata": {},
   "source": [
    "## Analyze Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4213383",
   "metadata": {},
   "source": [
    "14. Now that you have chunked your novel, you can analyze the chunk frequencies to gain insights!\n",
    "\n",
    "    A function `np_chunk_counter()` that returns the `30` most common NP-chunks from a list of chunked sentences has been imported for you in the code block for task 1. Call `np_chunk_counter()` with `np_chunked_text` as an argument and save the result to a variable named `most_common_np_chunks`. Print `most_common_np_chunks`. What sticks out to you about the most common noun phrase chunks? Are you surprised by anything? Open **Discover Insights into Classic Texts_Solution.ipynb** to see our analysis.\n",
    "    \n",
    "    Want to see how `np_chunk_counter()` works? Open **chunk_counters.ipynb** from the kit you downloaded and inspect `np_chunk_counter()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17916ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((('i', 'NN'),), 963), ((('henry', 'NN'),), 200), ((('lord', 'NN'),), 197), ((('life', 'NN'),), 170), ((('harry', 'NN'),), 136), ((('dorian', 'JJ'), ('gray', 'NN')), 127), ((('something', 'NN'),), 126), ((('nothing', 'NN'),), 93), ((('basil', 'NN'),), 85), ((('the', 'DT'), ('world', 'NN')), 70), ((('everything', 'NN'),), 69), ((('anything', 'NN'),), 68), ((('hallward', 'NN'),), 68), ((('the', 'DT'), ('man', 'NN')), 61), ((('the', 'DT'), ('room', 'NN')), 60), ((('face', 'NN'),), 57), ((('the', 'DT'), ('door', 'NN')), 56), ((('love', 'NN'),), 55), ((('art', 'NN'),), 52), ((('course', 'NN'),), 51), ((('the', 'DT'), ('picture', 'NN')), 46), ((('the', 'DT'), ('lad', 'NN')), 45), ((('head', 'NN'),), 44), ((('round', 'NN'),), 44), ((('hand', 'NN'),), 44), ((('sibyl', 'NN'),), 41), ((('the', 'DT'), ('table', 'NN')), 40), ((('the', 'DT'), ('painter', 'NN')), 38), ((('sir', 'NN'),), 38), ((('a', 'DT'), ('moment', 'NN')), 38)]\n"
     ]
    }
   ],
   "source": [
    "# store and print the most common NP-chunks here\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "print(most_common_np_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89a4b7",
   "metadata": {},
   "source": [
    "15. A function `vp_chunk_counter()` that returns the `30` most common VP-chunks from a list of chunked sentences has been imported for you in the code block for task 1. Call `vp_chunk_counter()` with `vp_chunked_text` as an argument and save the result to a variable named `most_common_vp_chunks`. Print `most_common_vp_chunks`. What sticks out to you about the most common verb phrase chunks? Are you surprised by anything? Open **Discover Insights into Classic Texts_Solution.ipynb** to see our analysis.\n",
    "\n",
    "    Want to see how `vp_chunk_counter()` works? Open **chunk_counters.ipynb** from the kit you downloaded and inspect `np_chunk_counter()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9cfdc4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((('i', 'NN'), ('am', 'VBP')), 101), ((('i', 'NN'), ('was', 'VBD')), 40), ((('i', 'NN'), ('want', 'VBP')), 37), ((('i', 'NN'), ('know', 'VBP')), 33), ((('i', 'NN'), ('do', 'VBP'), (\"n't\", 'RB')), 32), ((('i', 'NN'), ('have', 'VBP')), 32), ((('i', 'NN'), ('had', 'VBD')), 31), ((('i', 'NN'), ('suppose', 'VBP')), 17), ((('i', 'NN'), ('think', 'VBP')), 16), ((('i', 'NN'), ('am', 'VBP'), ('not', 'RB')), 14), ((('i', 'NN'), ('thought', 'VBD')), 13), ((('i', 'NN'), ('believe', 'VBP')), 12), ((('dorian', 'JJ'), ('gray', 'NN'), ('was', 'VBD')), 11), ((('i', 'NN'), ('am', 'VBP'), ('so', 'RB')), 11), ((('henry', 'NN'), ('had', 'VBD')), 11), ((('i', 'NN'), ('did', 'VBD'), (\"n't\", 'RB')), 9), ((('i', 'NN'), ('met', 'VBD')), 9), ((('i', 'NN'), ('said', 'VBD')), 9), ((('i', 'NN'), ('am', 'VBP'), ('quite', 'RB')), 8), ((('i', 'NN'), ('see', 'VBP')), 8), ((('i', 'NN'), ('did', 'VBD'), ('not', 'RB')), 7), ((('i', 'NN'), ('have', 'VBP'), ('ever', 'RB')), 7), ((('life', 'NN'), ('has', 'VBZ')), 7), ((('i', 'NN'), ('did', 'VBD')), 6), ((('i', 'NN'), ('feel', 'VBP')), 6), ((('life', 'NN'), ('is', 'VBZ')), 6), ((('the', 'DT'), ('lad', 'NN'), ('was', 'VBD')), 6), ((('i', 'NN'), ('asked', 'VBD')), 6), ((('i', 'NN'), ('came', 'VBD')), 6), ((('i', 'NN'), ('felt', 'VBD')), 6)]\n"
     ]
    }
   ],
   "source": [
    "# store and print the most common VP-chunks here\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
    "print(most_common_vp_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a9005",
   "metadata": {},
   "source": [
    "## Go Further On Your Own!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b125650e",
   "metadata": {},
   "source": [
    "16. Amazing! You have performed a syntax parsing analysis on a novel and gained insight into both the meaning of the text and how the author thinks, without reading a page!\n",
    "\n",
    "    Now's your chance to get creative. Is there a different pattern of parts-of-speech you want to identify and count in the novel you selected? Add a new piece of chunk grammar and repeat the process of chunking. What do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7cccd176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NV no/DT artist/NN is/VBZ) ever/RB morbid/VBN ./.)\n"
     ]
    }
   ],
   "source": [
    "# Creating a chunk grammar NV that represents a noun followed by a verb, with an optional determinator before \n",
    "nv_chunk_grammar = \"NV: {<DT>?<NN><VB.*>}\"\n",
    "nv_chunk_parser = RegexpParser(nv_chunk_grammar)\n",
    "nv_chunked_text = list()\n",
    "for pos_tagged_sentence in pos_tagged_text: \n",
    "    nv_chunked_text.append(nv_chunk_parser.parse(pos_tagged_sentence))\n",
    "\n",
    "print(nv_chunked_text[19])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66288d6d",
   "metadata": {},
   "source": [
    "17. Not the biggest fan of _The Picture of Dorian Gray_ or _The Iliad_? No worries! Included in the downloadable kit is a blank text file named `my_text.txt`. Open the file and copy any text of your choice (novel, script, article, etc.) into the file. Save the file and then return to this file (**Discover Insights into Classic Texts.ipynb**). Update the opened text file to `my_text.txt` and rerun this notebook to perform a syntax parsing analysis on your text! What insights or deeper meanings did you discover?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a960ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dfe52e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
